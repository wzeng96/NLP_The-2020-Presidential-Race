{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bernie = pd.read_csv(\"./Data/All_Candidates/Bernie_Sanders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hide highlightingFull TextTranslateUndo Translation FromToTranslateTranslation in progress... \\r\\n\\r\\n[[missing key: loadingAnimation]]The full text may take 40-120 seconds to translate; larger documents may take longer.\\r\\n\\r\\nCancel\\r\\nOverlayEndTurn on search term navigationTurn on search term navigation\\r\\n| Jump to first hitThe move by the group of young climate activists was another sign that left-wing advocacy organizations have increasingly coalesced around the Vermont senator\\'s candidacy.The Sunrise Movement, the collection of young climate activists who have roiled Capitol Hill and the Democratic presidential primary, announced on Thursday that it was endorsing Senator Bernie Sanders of Vermont, in another sign that left-wing advocacy groups have increasingly coalesced around his candidacy.In a landslide vote -- more than 75 percent of respondents -- Mr. Sanders earned the backing of members of the group, which has quickly become politically influential since its founding in 2017.Once a fledgling collection of college students, frustrated that Democrats and Republicans were not acting more quickly to curb climate change, Sunrise has grown to 318 chapters nationwide, with more than 10,000 members.Sunrise will host an event on Jan. 12 with Mr. Sanders in Iowa City to formally announce the endorsement. \"We believe a Bernie Sanders presidency would provide the best political terrain in which to engage in and ultimately win that struggle for the world we deserve,\" Varshini Prakash, a founder and the executive director of the movement, said in a statement. \"Senator Sanders has made it clear throughout his political career and in this campaign that he grasps the scale of the climate crisis, the urgency with which we must act to address it, and the opportunity we have in coming together to do so.\"The move represents another step into electoral politics for the group of young activists. After the 2018 midterm elections, the group made national headlines by staging a protest in the office of the incoming House speaker, Nancy Pelosi.Sunrise\\'s signature policy, the ambitious proposal known as the Green New Deal, became a crucial litmus test splitting moderates and liberals on climate change, embraced by top-tier candidates like Mr. Sanders, Senator Elizabeth Warren of Massachusetts and former Mayor Pete Buttigieg of South Bend, Ind. Even moderate candidates like former Vice President Joseph R. Biden Jr., the race\\'s current front-runner, have cited the policy as an inspiration and said its goal of drastically reducing greenhouse gas emissions while also addressing economic inequality was an important framework.But on the scorecard Sunrise released ranking the top three candidates\\' plans and support for the Green New Deal, Mr. Biden, with a score of 75, trailed far behind Mr. Sanders and Ms. Warren, who earned 183 and 171, respectively. Mr. Biden received low marks for how frequently he talks about the proposal and a 35 out of 100 for his \"Green New Deal vision.\"Mr. Biden also had a tense exchange with a Sunrise organizer in September, who pressed him on his commitment to environmental issues.\"Look at my record, child,\" Mr. Biden told the 18-year-old organizer.The group\\'s announcement is one of the last presidential endorsements to arrive from major progressive groups, almost all of which have backed Mr. Sanders. His candidacy in 2016 helped develop much of the left-wing political infrastructure in the Democratic Party -- and had huge support among young Democrats in particular -- but when the 2020 campaign cycle began, it was not inevitable that he would retain that support.Young Democratic voters had a bevy of options, including fresh faces like Mr. Buttigieg, former Representative Beto O\\'Rourke of Texas and Senator Kamala Harris of California. In Ms. Warren, there was another candidate with progressive bona fides -- who would also be the first woman in the Oval Office if elected.In September, the Working Families Party endorsed Ms. Warren over Mr. Sanders, saying that she was better positioned to create a cross-ideological coalition around liberal values and that grass-roots groups needed to choose a side in the primary.Over the next several months, however, and after Ms. Warren drew significant criticism for a health care proposal that stepped away from an immediate push for \"Medicare for all,\" the energy on the Democrats\\' left flank began to move away from her.In October, Mr. Sanders announced endorsements from popular House Democrats including Ilhan Omar of Minnesota and Alexandria Ocasio-Cortez of New York, who is a sponsor of the Green New Deal legislation. He has also gained the backing of labor organizations such as National Nurses United, and left-wing advocacy groups including the Center for Popular Democracy Action and People\\'s Action.This week, Mr. Sanders was endorsed by Dream Defenders, a Florida-based collection of activists that focuses on criminal justice reform.Sunrise gave its members two choices -- whether to back any presidential candidate at all, and if so, which one. More than 80 percent wanted to support a candidate. About 20 percent chose Ms. Warren.Ms. Warren sent Sunrise a positive message on Twitter in the wake of the announcement, restating her commitment to a Green New Deal. In recent weeks, with Sunrise likely to back Mr. Sanders, Ms. Warren announced she would hold a climate-focused town-hall-style event in New Hampshire and rolled out an endorsement from Rhiana Gunn-Wright, a lead author of the Green New Deal proposal.Ms. Warren has called for a $10.7 trillion investment in the economy to implement a Green New Deal and create what she calls green new jobs, and she has also released proposals specific to fighting climate change, including one that calls for $3 trillion in spending over a decade. Mr. Sanders released a $16.3 trillion plan for a Green New Deal that called for the United States to eliminate fossil fuel use by 2050 while similarly transforming the economy.Evan Weber, the political director for the Sunrise Movement, said the group\\'s advocacy would continue no matter who is the nominee.\"Should Senator Sanders become the next president of the United States, we will push to ensure that he make delivering upon the promise of his Green New Deal platform the top priority of his administration,\" he said. \"If Senator Sanders does not win the nomination, the stakes of the climate crisis also demand that we can\\'t sit this election out.\"PhotographSenator Bernie Sanders in California last month. His candidacy in 2016 helped develop much of the left-wing political infrastructure. (PHOTOGRAPH BY SEPTEMBER DAWN BOTTOMS FOR THE NEW YORK TIMES)Word count: 1005Show lessYou have requested \"on-the-fly\" machine translation of selected content from our databases. This functionality is provided solely for your convenience and is in no way intended to replace human translation. Show full disclaimerNeither ProQuest nor its licensors make any representations or warranties with respect to the translations. The translations are automatically generated \"AS IS\" and \"AS AVAILABLE\" and are not retained in our systems. PROQUEST AND ITS LICENSORS SPECIFICALLY DISCLAIM ANY AND ALL EXPRESS OR IMPLIED WARRANTIES, INCLUDING WITHOUT LIMITATION, ANY WARRANTIES FOR AVAILABILITY, ACCURACY, TIMELINESS, COMPLETENESS, NON-INFRINGMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Your use of the translations is subject to all use restrictions contained in your Electronic Products License Agreement and by using the translation functionality you agree to forgo any and all claims against ProQuest or its licensors for your use of the translation functionality and any output derived there from. Hide full disclaimer\\r\\n\\r\\nLonger documents can take a while to translate. Rather than keep you waiting, we have only translated the first few paragraphs. Click the button below if you want to translate the rest of the document.\\r\\nTranslate AllCopyright New York Times Company Jan 10, 2020'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernie['text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie['title'][0]\n",
    "# hand annotation indicies\n",
    "\n",
    "positive = [4, 11, 12, 18, 27, 29, 31, 33, 37, 42, 48, 51, 60, 67, 70, 74, 76, 80, 85, 89]\n",
    "\n",
    "negative = [0, 5, 10, 15, 20, 22, 23, 28, 30, 35, 39, 41, 43, 46, 49, 50, 59, 61, 63, 66]\n",
    "\n",
    "neutral = [1, 2, 3, 6, 7, 9, 13, 16, 17, 19, 21, 24, 26, 32, 37, 38, 45, 47, 52, 75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new col for sentiment\n",
    "\n",
    "bernie['sentiment'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate rows in new col with corresponding sentiment\n",
    "\n",
    "for i in bernie.index:\n",
    "    for j in positive:\n",
    "        if i == j:\n",
    "            bernie.at[i,'sentiment'] = 'pos'\n",
    "            \n",
    "for i in bernie.index:\n",
    "    for j in neutral:\n",
    "        if i == j:\n",
    "            bernie.at[i,'sentiment'] = 'neutral'\n",
    "            \n",
    "for i in bernie.index:\n",
    "    for j in negative:\n",
    "        if i == j:\n",
    "            bernie.at[i,'sentiment'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Modeling\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "\n",
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stopwords_english = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select text and news company names\n",
    "bernie_sentiment = bernie[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create corpus for each sentiment\n",
    "\n",
    "def create_corpus(bernie_sentiment, sentiment_name):\n",
    "    \n",
    "    df1 = bernie_sentiment.loc[bernie_sentiment['sentiment'] == sentiment_name]\n",
    "    #df2 = Sanders_news.loc[Sanders_news['media'] == media_name]\n",
    "    #df3 = Trump_news.loc[Trump_news['media'] == media_name]\n",
    "    #frames = [df1, df2, df3]\n",
    "    #df = pd.concat(frames, ignore_index = True)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small corpus for each sentiment\n",
    "POS = create_corpus(bernie_sentiment, sentiment_name = 'pos')\n",
    "NEG = create_corpus(bernie_sentiment, sentiment_name = 'neg')\n",
    "NEUTRAL = create_corpus(bernie_sentiment, sentiment_name = 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hide highlightingAbstractTranslateUndo Transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hide highlightingFull TextTranslateUndo Transl...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "1  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "2  Hide highlightingFull TextTranslateUndo Transl...       pos\n",
       "3  Hide highlightingAbstractTranslateUndo Transla...       pos\n",
       "4  Hide highlightingFull TextTranslateUndo Transl...       pos"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_All_sentiment = pd.concat([POS, NEG, NEUTRAL], axis = 0, ignore_index = True)\n",
    "corpus_All_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Preprocessing(corpus):\n",
    "    # convert string to list i.e. ['hide', 'highlightingfull', '[[missing']\n",
    "    corpus['text'] = corpus['text'].str.split()\n",
    "\n",
    "    # lower case each item in the list, and remove non-alphabetic characters i.e. ['hide', 'highlightingfull', 'missing']\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [re.sub(r'[^a-zA-Z]', \"\",y.lower()) for y in x])\n",
    "\n",
    "    # join the item in the list back to a string and replace keywords containing the target names\n",
    "#     keywords = ['new york times', 'the new york times', 'international new york times'\n",
    "#                 \"the washington post\", \"WP Company LLC\", \"washpostcom\",\n",
    "#                 'wall street journal', 'thomaswsjcom', 'Dow Jones Company Inc.']\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [' '.join(x)])\n",
    "\n",
    "    # stem each word in the text\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: str(x[0]))\n",
    "    corpus['text'] = corpus['text'].str.split()\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "    # join the item in the list back to a string\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: [' '.join(x)])\n",
    "\n",
    "    # convert list to a string\n",
    "    corpus['text'] = corpus['text'].apply(lambda x: str(x[0]))\n",
    "\n",
    "    print(type(corpus.iloc[0]['text']))\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide highlightingful texttranslateundo transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hide highlightingful texttranslateundo transla...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  hide highlightingful texttranslateundo transla...       pos\n",
       "1  hide highlightingful texttranslateundo transla...       pos"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentiment_corpus = Data_Preprocessing(corpus_All_sentiment)\n",
    "processed_sentiment_corpus.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46    hide highlightingful texttranslateundo transla...\n",
       "35    hide highlightingful texttranslateundo transla...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate features and targets\n",
    "X = processed_sentiment_corpus.iloc[:, 0]\n",
    "y = processed_sentiment_corpus.iloc[:, 1]\n",
    "\n",
    "# split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0, 'neutral': 1, 'pos': 2}\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "# get label name mapping\n",
    "le.fit(y_train)\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)\n",
    "\n",
    "# encode the target \n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Getting document term matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.2.1 Create matrix of token counts using unigram, bigram and trigram tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram matrix of token counts\n",
    "\n",
    "def get_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, \n",
    "                                 ngram_range = Ngram_range, binary=True) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram token counts matrix\n",
    "binary1_train, binary1_test = get_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram token counts matrix\n",
    "binary2_train, binary2_test = get_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram token counts matrix\n",
    "binary3_train, binary3_test = get_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in binary1_train is: 1665\n",
      "The unique terms in binary2_train is: 2824\n",
      "The unique terms in binary3_train is: 3179\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in binary1_train is:\", binary1_train.toarray().shape[1])\n",
    "print(\"The unique terms in binary2_train is:\", binary2_train.toarray().shape[1])\n",
    "print(\"The unique terms in binary3_train is:\", binary3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.2.2 Create DTM using unigram, bigram and trigram term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get unigram, bigram, and trigram term frequency matrix\n",
    "\n",
    "def get_TF_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = CountVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, ngram_range = Ngram_range) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram tf matrix\n",
    "tf1_train, tf1_test = get_TF_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram tf matrix\n",
    "tf2_train, tf2_test = get_TF_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram tf matrix\n",
    "tf3_train, tf3_test = get_TF_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in tf1_train is: 1665\n",
      "The unique terms in tf2_train is: 2824\n",
      "The unique terms in tf3_train is: 3179\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in tf1_train is:\", tf1_train.toarray().shape[1])\n",
    "print(\"The unique terms in tf2_train is:\", tf2_train.toarray().shape[1])\n",
    "print(\"The unique terms in tf3_train is:\", tf3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.2.3 Create DTM using unigram, bigram and trigram TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define a function to get unigram, bigram, and trigram TF-IDF matrix\n",
    "\n",
    "def get_TF_IDF_DTM(Ngram_range, x_train, x_test):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', min_df = int(3), max_df = 0.5, \n",
    "                                 ngram_range = Ngram_range) \n",
    "    vectorizer.fit(x_train)\n",
    "    trans_x_train = vectorizer.transform(x_train)\n",
    "    trans_x_test = vectorizer.transform(x_test)\n",
    "    \n",
    "    return trans_x_train, trans_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram tf-idf matrix\n",
    "tfidf1_train, tfidf1_test = get_TF_IDF_DTM(Ngram_range = (1, 1), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# bigram tf-idf matrix\n",
    "tfidf2_train, tfidf2_test = get_TF_IDF_DTM(Ngram_range = (1, 2), x_train = X_train, x_test = X_test)\n",
    "\n",
    "# trigram tf-idf matrix\n",
    "tfidf3_train, tfidf3_test = get_TF_IDF_DTM(Ngram_range = (1, 3), x_train = X_train, x_test = X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique terms in tfidf1_train is: 1665\n",
      "The unique terms in tfidf2_train is: 2824\n",
      "The unique terms in tfidf3_train is: 3179\n"
     ]
    }
   ],
   "source": [
    "print(\"The unique terms in tfidf1_train is:\", tfidf1_train.toarray().shape[1])\n",
    "print(\"The unique terms in tfidf2_train is:\", tfidf2_train.toarray().shape[1])\n",
    "print(\"The unique terms in tfidf3_train is:\", tfidf3_train.toarray().shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model training\n",
    "def train_model(clf, dtm, test):\n",
    "    # train data\n",
    "    clf.fit(dtm, y_train)\n",
    "    \n",
    "    # Predicting on the test set\n",
    "    preds = clf.predict(test)\n",
    "    \n",
    "    # print evaluation matrix\n",
    "    print(\"Accuracy:\", '{:1.4f}'.format(accuracy_score(y_test, preds)))\n",
    "    print(\"\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "    \n",
    "    return '{:1.4f}'.format(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram, binary\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.67      0.50      0.57         4\n",
      "           2       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.50      0.50      0.49        12\n",
      "weighted avg       0.50      0.50      0.49        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 0 1]\n",
      " [1 2 1]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "bigram, binary\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.67      0.50      0.57         4\n",
      "           2       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.50      0.50      0.49        12\n",
      "weighted avg       0.50      0.50      0.49        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 0 1]\n",
      " [1 2 1]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "trigram, binary\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.67      0.50      0.57         4\n",
      "           2       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.50      0.50      0.49        12\n",
      "weighted avg       0.50      0.50      0.49        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 0 1]\n",
      " [1 2 1]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF\n",
      "\n",
      "Accuracy: 0.4167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.50      0.44         4\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.41      0.42      0.41        12\n",
      "weighted avg       0.41      0.42      0.41        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 0 2]\n",
      " [2 2 0]\n",
      " [1 2 1]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.50      0.44         4\n",
      "           1       0.75      0.75      0.75         4\n",
      "           2       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.49      0.50      0.49        12\n",
      "weighted avg       0.49      0.50      0.49        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 0 2]\n",
      " [1 3 0]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "           2       0.33      0.25      0.29         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.48      0.50      0.48        12\n",
      "weighted avg       0.48      0.50      0.48        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 0 2]\n",
      " [1 3 0]\n",
      " [1 2 1]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.4167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         4\n",
      "           1       0.25      0.25      0.25         4\n",
      "           2       1.00      0.50      0.67         4\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.53      0.42      0.44        12\n",
      "weighted avg       0.53      0.42      0.44        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 2 0]\n",
      " [3 1 0]\n",
      " [1 1 2]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.3333\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.25      0.22         4\n",
      "           1       0.25      0.25      0.25         4\n",
      "           2       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.33        12\n",
      "   macro avg       0.37      0.33      0.35        12\n",
      "weighted avg       0.37      0.33      0.35        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 2 1]\n",
      " [3 1 0]\n",
      " [1 1 2]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.4167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         4\n",
      "           1       0.33      0.25      0.29         4\n",
      "           2       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.44      0.42      0.42        12\n",
      "weighted avg       0.44      0.42      0.42        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 1 1]\n",
      " [3 1 0]\n",
      " [1 1 2]]\n",
      "======================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use Naive Bayes\n",
    "#clf = XGBClassifier() #MultinomialNB()\n",
    "#clf = svm.SVC(gamma = 'scale', C = 1.0)\n",
    "#param = {'max_depth': 3, 'eta': 0.3, 'objective':'multi:softmax', 'num_class': 3}\n",
    "# param = {'max_depth': 3, 'learning_rate ': 0.3, 'objective':'multi:softmax'}\n",
    "#xgb_clf = XGBClassifier(param)\n",
    "#xgb_clf = XGBClassifier(max_depth=3, learning_rate=0.3, objective='multi:softmax')\n",
    "xgb_clf = XGBClassifier(max_depth=3, learning_rate=0.3, objective='multi:softmax', num_class=3)\n",
    "#svm_clf = svm.SVC(gamma = 'scale', C = 1.0)\n",
    "# reference: https://medium.com/@gabrielziegler3/multiclass-multilabel-classification-with-xgboost-66195e4d9f2d\n",
    "# reference: https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "# Model Configurations\n",
    "binary1 = (\"unigram, binary\", binary1_train, binary1_test)\n",
    "binary2 = (\"bigram, binary\",  binary2_train, binary2_test)\n",
    "binary3 = (\"trigram, binary\", binary3_train, binary3_test)\n",
    "tf1 = (\"unigram, TF\", tf1_train, tf1_test)\n",
    "tf2 = (\"bigram, TF\",  tf2_train, tf2_test)\n",
    "tf3 = (\"trigram, TF\", tf3_train, tf3_test)\n",
    "tfidf1 = (\"unigram, TF-IDF\", tfidf1_train, tfidf1_test)\n",
    "tfidf2 = (\"bigram, TF-IDF\",  tfidf2_train, tfidf2_test)\n",
    "tfidf3 = (\"trigram, TF-IDF\", tfidf3_train, tfidf3_test)\n",
    "DTMs = [binary1, binary2, binary3,\n",
    "        tf1, tf2, tf3,\n",
    "        tfidf1, tfidf2, tfidf3]\n",
    "\n",
    "df = pd.DataFrame({\"config\": [],\n",
    "                   \"accuracy\": []})\n",
    "best_config = [\"Best Configuration\", \"none\", 0, \"none\", \"none\"]\n",
    "for data in DTMs:\n",
    "    print(data[0])\n",
    "    print(\"\")\n",
    "    score = train_model(clf = xgb_clf, dtm = data[1], test = data[2])\n",
    "    print(\"======================================================\")\n",
    "    print(\"\")\n",
    "    if float(score) > float(best_config[2]):\n",
    "        best_config = [\"Best Configuration:\", data[0], score, data[1], data[2]]\n",
    "    df = df.append({\"config\": data[0],\n",
    "               \"accuracy\": float(score)},\n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.3.2 SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram, binary\n",
      "\n",
      "Accuracy: 0.3333\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         4\n",
      "           1       0.33      0.50      0.40         4\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.33        12\n",
      "   macro avg       0.22      0.33      0.27        12\n",
      "weighted avg       0.22      0.33      0.27        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 2 0]\n",
      " [2 2 0]\n",
      " [2 2 0]]\n",
      "======================================================\n",
      "\n",
      "bigram, binary\n",
      "\n",
      "Accuracy: 0.4167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.50      0.44         4\n",
      "           1       0.43      0.75      0.55         4\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.28      0.42      0.33        12\n",
      "weighted avg       0.28      0.42      0.33        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 2 0]\n",
      " [1 3 0]\n",
      " [2 2 0]]\n",
      "======================================================\n",
      "\n",
      "trigram, binary\n",
      "\n",
      "Accuracy: 0.4167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         4\n",
      "           1       0.38      0.75      0.50         4\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.29      0.42      0.33        12\n",
      "weighted avg       0.29      0.42      0.33        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 2 0]\n",
      " [1 3 0]\n",
      " [1 3 0]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF\n",
      "\n",
      "Accuracy: 0.5000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      1.00      0.62         4\n",
      "           1       0.50      0.25      0.33         4\n",
      "           2       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.50        12\n",
      "   macro avg       0.65      0.50      0.45        12\n",
      "weighted avg       0.65      0.50      0.45        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4 0 0]\n",
      " [3 1 0]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF\n",
      "\n",
      "Accuracy: 0.4167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.75      0.50         4\n",
      "           1       0.33      0.25      0.29         4\n",
      "           2       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.57      0.42      0.40        12\n",
      "weighted avg       0.57      0.42      0.40        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [3 1 0]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF\n",
      "\n",
      "Accuracy: 0.4167\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.75      0.50         4\n",
      "           1       0.33      0.25      0.29         4\n",
      "           2       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.42        12\n",
      "   macro avg       0.57      0.42      0.40        12\n",
      "weighted avg       0.57      0.42      0.40        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [3 1 0]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "unigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "           2       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.70      0.58      0.56        12\n",
      "weighted avg       0.70      0.58      0.56        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [1 3 0]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "bigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "           2       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.70      0.58      0.56        12\n",
      "weighted avg       0.70      0.58      0.56        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [1 3 0]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n",
      "trigram, TF-IDF\n",
      "\n",
      "Accuracy: 0.5833\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.75      0.60         4\n",
      "           1       0.60      0.75      0.67         4\n",
      "           2       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.58        12\n",
      "   macro avg       0.70      0.58      0.56        12\n",
      "weighted avg       0.70      0.58      0.56        12\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3 1 0]\n",
      " [1 3 0]\n",
      " [2 1 1]]\n",
      "======================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Download\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Download\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Download\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# svm classifier\n",
    "df_svm = pd.DataFrame({\"config\": [],\n",
    "                   \"accuracy\": []})\n",
    "best_config_svm = [\"Best Configuration\", \"none\", 0, \"none\", \"none\"]\n",
    "for data in DTMs:\n",
    "    print(data[0])\n",
    "    print(\"\")\n",
    "    score = train_model(clf = svm_clf, dtm = data[1], test = data[2])\n",
    "    print(\"======================================================\")\n",
    "    print(\"\")\n",
    "    if float(score) > float(best_config_svm[2]):\n",
    "        best_config_svm = [\"Best Configuration:\", data[0], score, data[1], data[2]]\n",
    "    df_svm = df_svm.append({\"config\": data[0],\n",
    "               \"accuracy\": float(score)},\n",
    "               ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>config</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unigram, binary</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bigram, binary</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trigram, binary</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unigram, TF</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bigram, TF</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trigram, TF</td>\n",
       "      <td>0.4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unigram, TF-IDF</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bigram, TF-IDF</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trigram, TF-IDF</td>\n",
       "      <td>0.5833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            config  accuracy\n",
       "0  unigram, binary    0.3333\n",
       "1   bigram, binary    0.4167\n",
       "2  trigram, binary    0.4167\n",
       "3      unigram, TF    0.5000\n",
       "4       bigram, TF    0.4167\n",
       "5      trigram, TF    0.4167\n",
       "6  unigram, TF-IDF    0.5833\n",
       "7   bigram, TF-IDF    0.5833\n",
       "8  trigram, TF-IDF    0.5833"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results of svm classifier\n",
    "df_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Best Configuration:', 'unigram, TF-IDF', '0.5833', <47x1665 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 12342 stored elements in Compressed Sparse Row format>, <12x1665 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 3086 stored elements in Compressed Sparse Row format>]\n"
     ]
    }
   ],
   "source": [
    "# best model\n",
    "print(best_config_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
